{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chizkidd/microgpt?scriptVersionId=297786989\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 1. Karparthy's microGPT version\n---\n\nIn his own words per his [blog](https://karpathy.github.io/):\n\n>_This is a brief guide to my new art project microgpt, a single file of 200 lines of pure Python with no dependencies that trains and inferences a GPT. This file contains the full algorithmic content of what is needed: dataset of documents, tokenizer, autograd engine, a GPT-2-like neural network architecture, the Adam optimizer, training loop, and inference loop. Everything else is just efficiency. I cannot simplify this any further. This script is the culmination of multiple projects (micrograd, makemore, nanogpt, etc.) and a decade-long obsession to simplify LLMs to their bare essentials, and I think it is beautiful ðŸ¥¹_\n\n- Tweet: https://x.com/karpathy/status/2021694437152157847\n- Blog: https://karpathy.github.io/2026/02/12/microgpt/\n- Code: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThe most atomic way to train and inference a GPT in pure, dependency-free Python.\nThis file is the complete algorithm.\nEverything else is just efficiency.\n\n@karpathy\n\"\"\"\n\nimport os       # os.path.exists\nimport math     # math.log, math.exp\nimport random   # random.seed, random.choices, random.gauss, random.shuffle\nrandom.seed(42) # Let there be order among chaos\n\n# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)\nif not os.path.exists('input.txt'):\n    import urllib.request\n    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n    urllib.request.urlretrieve(names_url, 'input.txt')\ndocs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents\nrandom.shuffle(docs)\nprint(f\"num docs: {len(docs)}\")\n\n# Let there be a Tokenizer to translate strings to discrete symbols and back\nuchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\nBOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\nvocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\nprint(f\"vocab size: {vocab_size}\")\n\n# Let there be Autograd, to recursively apply the chain rule through a computation graph\nclass Value:\n    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n\n    def __init__(self, data, children=(), local_grads=()):\n        self.data = data                # scalar value of this node calculated during forward pass\n        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n        self._children = children       # children of this node in the computation graph\n        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        return Value(self.data + other.data, (self, other), (1, 1))\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        return Value(self.data * other.data, (self, other), (other.data, self.data))\n\n    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n    def __neg__(self): return self * -1\n    def __radd__(self, other): return self + other\n    def __sub__(self, other): return self + (-other)\n    def __rsub__(self, other): return other + (-self)\n    def __rmul__(self, other): return self * other\n    def __truediv__(self, other): return self * other**-1\n    def __rtruediv__(self, other): return other * self**-1\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._children:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            for child, local_grad in zip(v._children, v._local_grads):\n                child.grad += local_grad * v.grad\n\n# Initialize the parameters, to store the knowledge of the model.\nn_embd = 16     # embedding dimension\nn_head = 4      # number of attention heads\nn_layer = 1     # number of layers\nblock_size = 16 # maximum sequence length\nhead_dim = n_embd // n_head # dimension of each head\nmatrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\nstate_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\nfor i in range(n_layer):\n    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\nparams = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\nprint(f\"num params: {len(params)}\")\n\n# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\n# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\ndef linear(x, w):\n    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n\ndef softmax(logits):\n    max_val = max(val.data for val in logits)\n    exps = [(val - max_val).exp() for val in logits]\n    total = sum(exps)\n    return [e / total for e in exps]\n\ndef rmsnorm(x):\n    ms = sum(xi * xi for xi in x) / len(x)\n    scale = (ms + 1e-5) ** -0.5\n    return [xi * scale for xi in x]\n\ndef gpt(token_id, pos_id, keys, values):\n    tok_emb = state_dict['wte'][token_id] # token embedding\n    pos_emb = state_dict['wpe'][pos_id] # position embedding\n    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n    x = rmsnorm(x)\n\n    for li in range(n_layer):\n        # 1) Multi-head attention block\n        x_residual = x\n        x = rmsnorm(x)\n        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n        keys[li].append(k)\n        values[li].append(v)\n        x_attn = []\n        for h in range(n_head):\n            hs = h * head_dim\n            q_h = q[hs:hs+head_dim]\n            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n            attn_weights = softmax(attn_logits)\n            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n            x_attn.extend(head_out)\n        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n        x = [a + b for a, b in zip(x, x_residual)]\n        # 2) MLP block\n        x_residual = x\n        x = rmsnorm(x)\n        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n        x = [xi.relu() for xi in x]\n        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n        x = [a + b for a, b in zip(x, x_residual)]\n\n    logits = linear(x, state_dict['lm_head'])\n    return logits\n\n# Let there be Adam, the blessed optimizer and its buffers\nlearning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\nm = [0.0] * len(params) # first moment buffer\nv = [0.0] * len(params) # second moment buffer\n\nimport time\ntrain_start = time.time()\n# Repeat in sequence\nnum_steps = 1000 # number of training steps\nfor step in range(num_steps):\n\n    # Take single document, tokenize it, surround it with BOS special token on both sides\n    doc = docs[step % len(docs)]\n    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n    n = min(block_size, len(tokens) - 1)\n\n    # Forward the token sequence through the model, building up the computation graph all the way to the loss.\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    losses = []\n    for pos_id in range(n):\n        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax(logits)\n        loss_t = -probs[target_id].log()\n        losses.append(loss_t)\n    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n\n    # Backward the loss, calculating the gradients with respect to all model parameters.\n    loss.backward()\n\n    # Adam optimizer update: update the model parameters based on the corresponding gradients.\n    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n    for i, p in enumerate(params):\n        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n        m_hat = m[i] / (1 - beta1 ** (step + 1))\n        v_hat = v[i] / (1 - beta2 ** (step + 1))\n        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n        p.grad = 0\n\n    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n\ntrain_end = time.time()\n\n# Inference: may the model babble back to us\ntemperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\nprint(\"\\n--- inference (new, hallucinated names) ---\")\nfor sample_idx in range(20):\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    token_id = BOS\n    sample = []\n    for pos_id in range(block_size):\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax([l / temperature for l in logits])\n        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n        if token_id == BOS:\n            break\n        sample.append(uchars[token_id])\n    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")\n\n\nfinal_end = time.time()\n\nprint(f\"train start:  {time.strftime('%H:%M:%S', time.localtime(train_start))}\")\nprint(f\"train end:    {time.strftime('%H:%M:%S', time.localtime(train_end))}\")\nprint(f\"final end:    {time.strftime('%H:%M:%S', time.localtime(final_end))}\")\nprint(f\"train time:   {train_end - train_start:.1f}s  ({(train_end - train_start)/60:.1f} min)\")\nprint(f\"total time:   {final_end - train_start:.1f}s  ({(final_end - train_start)/60:.1f} min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T01:53:03.560637Z","iopub.execute_input":"2026-02-15T01:53:03.561246Z","iopub.status.idle":"2026-02-15T01:56:25.10332Z","shell.execute_reply.started":"2026-02-15T01:53:03.561211Z","shell.execute_reply":"2026-02-15T01:56:25.102661Z"}},"outputs":[{"name":"stdout","text":"num docs: 32033\nvocab size: 27\nnum params: 4192\nstep    1 / 1000 | loss 3.3660\nstep    2 / 1000 | loss 3.4243\nstep    3 / 1000 | loss 3.1778\nstep    4 / 1000 | loss 3.0664\nstep    5 / 1000 | loss 3.2209\nstep    6 / 1000 | loss 2.9452\nstep    7 / 1000 | loss 3.2894\nstep    8 / 1000 | loss 3.3245\nstep    9 / 1000 | loss 2.8990\nstep   10 / 1000 | loss 3.2229\nstep   11 / 1000 | loss 2.7964\nstep   12 / 1000 | loss 2.9345\nstep   13 / 1000 | loss 3.0544\nstep   14 / 1000 | loss 3.0905\nstep   15 / 1000 | loss 3.0651\nstep   16 / 1000 | loss 2.7337\nstep   17 / 1000 | loss 2.8839\nstep   18 / 1000 | loss 2.8977\nstep   19 / 1000 | loss 2.7073\nstep   20 / 1000 | loss 2.7453\nstep   21 / 1000 | loss 3.7212\nstep   22 / 1000 | loss 2.8026\nstep   23 / 1000 | loss 2.8241\nstep   24 / 1000 | loss 2.0374\nstep   25 / 1000 | loss 3.3698\nstep   26 / 1000 | loss 2.9154\nstep   27 / 1000 | loss 3.2795\nstep   28 / 1000 | loss 2.9195\nstep   29 / 1000 | loss 2.3027\nstep   30 / 1000 | loss 2.2691\nstep   31 / 1000 | loss 2.8957\nstep   32 / 1000 | loss 2.9539\nstep   33 / 1000 | loss 2.6819\nstep   34 / 1000 | loss 2.1899\nstep   35 / 1000 | loss 3.1121\nstep   36 / 1000 | loss 2.7269\nstep   37 / 1000 | loss 2.4928\nstep   38 / 1000 | loss 2.9746\nstep   39 / 1000 | loss 2.2992\nstep   40 / 1000 | loss 2.8604\nstep   41 / 1000 | loss 2.3052\nstep   42 / 1000 | loss 2.5615\nstep   43 / 1000 | loss 2.9018\nstep   44 / 1000 | loss 2.4472\nstep   45 / 1000 | loss 2.1513\nstep   46 / 1000 | loss 3.0613\nstep   47 / 1000 | loss 2.5581\nstep   48 / 1000 | loss 3.0171\nstep   49 / 1000 | loss 2.6902\nstep   50 / 1000 | loss 2.4050\nstep   51 / 1000 | loss 3.6813\nstep   52 / 1000 | loss 2.8990\nstep   53 / 1000 | loss 3.0358\nstep   54 / 1000 | loss 2.2217\nstep   55 / 1000 | loss 2.7366\nstep   56 / 1000 | loss 2.2113\nstep   57 / 1000 | loss 2.6736\nstep   58 / 1000 | loss 2.4947\nstep   59 / 1000 | loss 2.6330\nstep   60 / 1000 | loss 2.9024\nstep   61 / 1000 | loss 2.6594\nstep   62 / 1000 | loss 2.4527\nstep   63 / 1000 | loss 2.7178\nstep   64 / 1000 | loss 2.8619\nstep   65 / 1000 | loss 2.8474\nstep   66 / 1000 | loss 2.8673\nstep   67 / 1000 | loss 2.7473\nstep   68 / 1000 | loss 2.5459\nstep   69 / 1000 | loss 2.5597\nstep   70 / 1000 | loss 2.8365\nstep   71 / 1000 | loss 3.4163\nstep   72 / 1000 | loss 2.5205\nstep   73 / 1000 | loss 2.5853\nstep   74 / 1000 | loss 2.4236\nstep   75 / 1000 | loss 2.4053\nstep   76 / 1000 | loss 2.7836\nstep   77 / 1000 | loss 2.8438\nstep   78 / 1000 | loss 3.0302\nstep   79 / 1000 | loss 2.3869\nstep   80 / 1000 | loss 2.3910\nstep   81 / 1000 | loss 2.3688\nstep   82 / 1000 | loss 3.1079\nstep   83 / 1000 | loss 2.5942\nstep   84 / 1000 | loss 2.1857\nstep   85 / 1000 | loss 2.1898\nstep   86 / 1000 | loss 2.4624\nstep   87 / 1000 | loss 2.9832\nstep   88 / 1000 | loss 2.7304\nstep   89 / 1000 | loss 2.6506\nstep   90 / 1000 | loss 2.5744\nstep   91 / 1000 | loss 2.5922\nstep   92 / 1000 | loss 3.1646\nstep   93 / 1000 | loss 2.9175\nstep   94 / 1000 | loss 2.9945\nstep   95 / 1000 | loss 2.3016\nstep   96 / 1000 | loss 2.4319\nstep   97 / 1000 | loss 2.0333\nstep   98 / 1000 | loss 3.3962\nstep   99 / 1000 | loss 2.2613\nstep  100 / 1000 | loss 3.3669\nstep  101 / 1000 | loss 2.4483\nstep  102 / 1000 | loss 2.2889\nstep  103 / 1000 | loss 2.7991\nstep  104 / 1000 | loss 2.6872\nstep  105 / 1000 | loss 2.6311\nstep  106 / 1000 | loss 2.4243\nstep  107 / 1000 | loss 2.8984\nstep  108 / 1000 | loss 2.2613\nstep  109 / 1000 | loss 2.2090\nstep  110 / 1000 | loss 2.5113\nstep  111 / 1000 | loss 2.6165\nstep  112 / 1000 | loss 2.6483\nstep  113 / 1000 | loss 2.6772\nstep  114 / 1000 | loss 2.2588\nstep  115 / 1000 | loss 2.2152\nstep  116 / 1000 | loss 2.8152\nstep  117 / 1000 | loss 2.6372\nstep  118 / 1000 | loss 2.0875\nstep  119 / 1000 | loss 2.5167\nstep  120 / 1000 | loss 2.6920\nstep  121 / 1000 | loss 2.3495\nstep  122 / 1000 | loss 2.2998\nstep  123 / 1000 | loss 2.7507\nstep  124 / 1000 | loss 2.5124\nstep  125 / 1000 | loss 3.0075\nstep  126 / 1000 | loss 2.2402\nstep  127 / 1000 | loss 2.6489\nstep  128 / 1000 | loss 2.2248\nstep  129 / 1000 | loss 2.1412\nstep  130 / 1000 | loss 3.0851\nstep  131 / 1000 | loss 2.8208\nstep  132 / 1000 | loss 2.0810\nstep  133 / 1000 | loss 2.8060\nstep  134 / 1000 | loss 2.7096\nstep  135 / 1000 | loss 2.6401\nstep  136 / 1000 | loss 3.1040\nstep  137 / 1000 | loss 2.1829\nstep  138 / 1000 | loss 2.2031\nstep  139 / 1000 | loss 2.7783\nstep  140 / 1000 | loss 2.5121\nstep  141 / 1000 | loss 3.2092\nstep  142 / 1000 | loss 2.3187\nstep  143 / 1000 | loss 3.1077\nstep  144 / 1000 | loss 2.2708\nstep  145 / 1000 | loss 2.6134\nstep  146 / 1000 | loss 2.5703\nstep  147 / 1000 | loss 2.2524\nstep  148 / 1000 | loss 2.3684\nstep  149 / 1000 | loss 2.1724\nstep  150 / 1000 | loss 2.5351\nstep  151 / 1000 | loss 2.9063\nstep  152 / 1000 | loss 2.4862\nstep  153 / 1000 | loss 2.8622\nstep  154 / 1000 | loss 3.1861\nstep  155 / 1000 | loss 2.5595\nstep  156 / 1000 | loss 2.8595\nstep  157 / 1000 | loss 3.2600\nstep  158 / 1000 | loss 2.0037\nstep  159 / 1000 | loss 3.3867\nstep  160 / 1000 | loss 2.1104\nstep  161 / 1000 | loss 2.2022\nstep  162 / 1000 | loss 2.6278\nstep  163 / 1000 | loss 2.5539\nstep  164 / 1000 | loss 2.4285\nstep  165 / 1000 | loss 2.3338\nstep  166 / 1000 | loss 2.6923\nstep  167 / 1000 | loss 2.1427\nstep  168 / 1000 | loss 2.5276\nstep  169 / 1000 | loss 3.1430\nstep  170 / 1000 | loss 2.5338\nstep  171 / 1000 | loss 2.6454\nstep  172 / 1000 | loss 2.3900\nstep  173 / 1000 | loss 2.2324\nstep  174 / 1000 | loss 3.0033\nstep  175 / 1000 | loss 2.6798\nstep  176 / 1000 | loss 2.5880\nstep  177 / 1000 | loss 3.1046\nstep  178 / 1000 | loss 2.3841\nstep  179 / 1000 | loss 1.9982\nstep  180 / 1000 | loss 2.3556\nstep  181 / 1000 | loss 2.1472\nstep  182 / 1000 | loss 2.0537\nstep  183 / 1000 | loss 1.9403\nstep  184 / 1000 | loss 3.3390\nstep  185 / 1000 | loss 2.1482\nstep  186 / 1000 | loss 2.4919\nstep  187 / 1000 | loss 2.4610\nstep  188 / 1000 | loss 2.4055\nstep  189 / 1000 | loss 1.9792\nstep  190 / 1000 | loss 2.6377\nstep  191 / 1000 | loss 1.7000\nstep  192 / 1000 | loss 2.4035\nstep  193 / 1000 | loss 2.2961\nstep  194 / 1000 | loss 2.8886\nstep  195 / 1000 | loss 2.8026\nstep  196 / 1000 | loss 2.4264\nstep  197 / 1000 | loss 2.3991\nstep  198 / 1000 | loss 3.0697\nstep  199 / 1000 | loss 2.5300\nstep  200 / 1000 | loss 2.3097\nstep  201 / 1000 | loss 2.4874\nstep  202 / 1000 | loss 2.5649\nstep  203 / 1000 | loss 2.1233\nstep  204 / 1000 | loss 1.8898\nstep  205 / 1000 | loss 2.6302\nstep  206 / 1000 | loss 3.1559\nstep  207 / 1000 | loss 2.8998\nstep  208 / 1000 | loss 2.1443\nstep  209 / 1000 | loss 2.2206\nstep  210 / 1000 | loss 2.5670\nstep  211 / 1000 | loss 2.0186\nstep  212 / 1000 | loss 2.3012\nstep  213 / 1000 | loss 3.8427\nstep  214 / 1000 | loss 2.2129\nstep  215 / 1000 | loss 2.4124\nstep  216 / 1000 | loss 2.5136\nstep  217 / 1000 | loss 2.3378\nstep  218 / 1000 | loss 2.5365\nstep  219 / 1000 | loss 2.3739\nstep  220 / 1000 | loss 2.4205\nstep  221 / 1000 | loss 3.0695\nstep  222 / 1000 | loss 2.3135\nstep  223 / 1000 | loss 2.1625\nstep  224 / 1000 | loss 2.3273\nstep  225 / 1000 | loss 2.2527\nstep  226 / 1000 | loss 2.4193\nstep  227 / 1000 | loss 2.4528\nstep  228 / 1000 | loss 2.5524\nstep  229 / 1000 | loss 3.0859\nstep  230 / 1000 | loss 1.7900\nstep  231 / 1000 | loss 3.1017\nstep  232 / 1000 | loss 2.4001\nstep  233 / 1000 | loss 2.3035\nstep  234 / 1000 | loss 2.7662\nstep  235 / 1000 | loss 2.0570\nstep  236 / 1000 | loss 2.7383\nstep  237 / 1000 | loss 2.2569\nstep  238 / 1000 | loss 2.6960\nstep  239 / 1000 | loss 2.4001\nstep  240 / 1000 | loss 3.6365\nstep  241 / 1000 | loss 2.8041\nstep  242 / 1000 | loss 2.5392\nstep  243 / 1000 | loss 2.3092\nstep  244 / 1000 | loss 2.6435\nstep  245 / 1000 | loss 2.2066\nstep  246 / 1000 | loss 2.7219\nstep  247 / 1000 | loss 2.4871\nstep  248 / 1000 | loss 2.7047\nstep  249 / 1000 | loss 2.0570\nstep  250 / 1000 | loss 2.1581\nstep  251 / 1000 | loss 1.9875\nstep  252 / 1000 | loss 2.4351\nstep  253 / 1000 | loss 2.7340\nstep  254 / 1000 | loss 1.9832\nstep  255 / 1000 | loss 2.4915\nstep  256 / 1000 | loss 3.5044\nstep  257 / 1000 | loss 2.3991\nstep  258 / 1000 | loss 1.8618\nstep  259 / 1000 | loss 1.9200\nstep  260 / 1000 | loss 1.7671\nstep  261 / 1000 | loss 2.6093\nstep  262 / 1000 | loss 2.2438\nstep  263 / 1000 | loss 2.9581\nstep  264 / 1000 | loss 3.0106\nstep  265 / 1000 | loss 1.8756\nstep  266 / 1000 | loss 2.7724\nstep  267 / 1000 | loss 1.9729\nstep  268 / 1000 | loss 2.1480\nstep  269 / 1000 | loss 2.1096\nstep  270 / 1000 | loss 2.8207\nstep  271 / 1000 | loss 2.2624\nstep  272 / 1000 | loss 1.9211\nstep  273 / 1000 | loss 2.6192\nstep  274 / 1000 | loss 3.0047\nstep  275 / 1000 | loss 2.0174\nstep  276 / 1000 | loss 2.5915\nstep  277 / 1000 | loss 3.1114\nstep  278 / 1000 | loss 2.3490\nstep  279 / 1000 | loss 2.3004\nstep  280 / 1000 | loss 1.9486\nstep  281 / 1000 | loss 3.1744\nstep  282 / 1000 | loss 1.9351\nstep  283 / 1000 | loss 2.4215\nstep  284 / 1000 | loss 2.7351\nstep  285 / 1000 | loss 3.3271\nstep  286 / 1000 | loss 2.1280\nstep  287 / 1000 | loss 2.3728\nstep  288 / 1000 | loss 2.5311\nstep  289 / 1000 | loss 2.4675\nstep  290 / 1000 | loss 2.1163\nstep  291 / 1000 | loss 3.0499\nstep  292 / 1000 | loss 2.3976\nstep  293 / 1000 | loss 1.9984\nstep  294 / 1000 | loss 2.5432\nstep  295 / 1000 | loss 2.7180\nstep  296 / 1000 | loss 2.1555\nstep  297 / 1000 | loss 2.3680\nstep  298 / 1000 | loss 2.6502\nstep  299 / 1000 | loss 2.1947\nstep  300 / 1000 | loss 2.3178\nstep  301 / 1000 | loss 2.6931\nstep  302 / 1000 | loss 2.1736\nstep  303 / 1000 | loss 2.6196\nstep  304 / 1000 | loss 2.3674\nstep  305 / 1000 | loss 2.8884\nstep  306 / 1000 | loss 2.5560\nstep  307 / 1000 | loss 1.9077\nstep  308 / 1000 | loss 2.5663\nstep  309 / 1000 | loss 2.0727\nstep  310 / 1000 | loss 2.3818\nstep  311 / 1000 | loss 3.0383\nstep  312 / 1000 | loss 2.0074\nstep  313 / 1000 | loss 1.7555\nstep  314 / 1000 | loss 2.3456\nstep  315 / 1000 | loss 2.6081\nstep  316 / 1000 | loss 2.0439\nstep  317 / 1000 | loss 2.0600\nstep  318 / 1000 | loss 1.8657\nstep  319 / 1000 | loss 1.9699\nstep  320 / 1000 | loss 1.7969\nstep  321 / 1000 | loss 2.5492\nstep  322 / 1000 | loss 2.4804\nstep  323 / 1000 | loss 2.7345\nstep  324 / 1000 | loss 3.1487\nstep  325 / 1000 | loss 2.4556\nstep  326 / 1000 | loss 2.0597\nstep  327 / 1000 | loss 2.3248\nstep  328 / 1000 | loss 1.9027\nstep  329 / 1000 | loss 2.1499\nstep  330 / 1000 | loss 2.3627\nstep  331 / 1000 | loss 2.1251\nstep  332 / 1000 | loss 2.4151\nstep  333 / 1000 | loss 1.9030\nstep  334 / 1000 | loss 2.8525\nstep  335 / 1000 | loss 3.9066\nstep  336 / 1000 | loss 3.3516\nstep  337 / 1000 | loss 2.6985\nstep  338 / 1000 | loss 2.5921\nstep  339 / 1000 | loss 2.2606\nstep  340 / 1000 | loss 2.1589\nstep  341 / 1000 | loss 3.0280\nstep  342 / 1000 | loss 2.8235\nstep  343 / 1000 | loss 1.8070\nstep  344 / 1000 | loss 2.5350\nstep  345 / 1000 | loss 2.4687\nstep  346 / 1000 | loss 2.4156\nstep  347 / 1000 | loss 2.9995\nstep  348 / 1000 | loss 2.4981\nstep  349 / 1000 | loss 2.6259\nstep  350 / 1000 | loss 2.2592\nstep  351 / 1000 | loss 3.1636\nstep  352 / 1000 | loss 1.9862\nstep  353 / 1000 | loss 2.3807\nstep  354 / 1000 | loss 2.8480\nstep  355 / 1000 | loss 2.5412\nstep  356 / 1000 | loss 2.1290\nstep  357 / 1000 | loss 2.7031\nstep  358 / 1000 | loss 2.0749\nstep  359 / 1000 | loss 1.9376\nstep  360 / 1000 | loss 2.4932\nstep  361 / 1000 | loss 2.5539\nstep  362 / 1000 | loss 2.4702\nstep  363 / 1000 | loss 1.8193\nstep  364 / 1000 | loss 1.9877\nstep  365 / 1000 | loss 2.6337\nstep  366 / 1000 | loss 1.9184\nstep  367 / 1000 | loss 3.0730\nstep  368 / 1000 | loss 2.5106\nstep  369 / 1000 | loss 2.8109\nstep  370 / 1000 | loss 2.0459\nstep  371 / 1000 | loss 2.9865\nstep  372 / 1000 | loss 2.1616\nstep  373 / 1000 | loss 2.7536\nstep  374 / 1000 | loss 2.1206\nstep  375 / 1000 | loss 1.9970\nstep  376 / 1000 | loss 2.4778\nstep  377 / 1000 | loss 2.3444\nstep  378 / 1000 | loss 2.2609\nstep  379 / 1000 | loss 2.4662\nstep  380 / 1000 | loss 2.2087\nstep  381 / 1000 | loss 2.4502\nstep  382 / 1000 | loss 2.7536\nstep  383 / 1000 | loss 2.3231\nstep  384 / 1000 | loss 3.2495\nstep  385 / 1000 | loss 2.9181\nstep  386 / 1000 | loss 2.3336\nstep  387 / 1000 | loss 3.6985\nstep  388 / 1000 | loss 2.2499\nstep  389 / 1000 | loss 2.3085\nstep  390 / 1000 | loss 3.1236\nstep  391 / 1000 | loss 2.4739\nstep  392 / 1000 | loss 2.1051\nstep  393 / 1000 | loss 2.1702\nstep  394 / 1000 | loss 2.2743\nstep  395 / 1000 | loss 2.6582\nstep  396 / 1000 | loss 1.8241\nstep  397 / 1000 | loss 2.0875\nstep  398 / 1000 | loss 2.8767\nstep  399 / 1000 | loss 2.7444\nstep  400 / 1000 | loss 2.3428\nstep  401 / 1000 | loss 2.6035\nstep  402 / 1000 | loss 2.7292\nstep  403 / 1000 | loss 1.9550\nstep  404 / 1000 | loss 2.2429\nstep  405 / 1000 | loss 2.7119\nstep  406 / 1000 | loss 2.5498\nstep  407 / 1000 | loss 2.2875\nstep  408 / 1000 | loss 2.6208\nstep  409 / 1000 | loss 2.8385\nstep  410 / 1000 | loss 2.9415\nstep  411 / 1000 | loss 2.2064\nstep  412 / 1000 | loss 2.1636\nstep  413 / 1000 | loss 2.2308\nstep  414 / 1000 | loss 2.8363\nstep  415 / 1000 | loss 2.0398\nstep  416 / 1000 | loss 2.4377\nstep  417 / 1000 | loss 2.9288\nstep  418 / 1000 | loss 1.9164\nstep  419 / 1000 | loss 2.4943\nstep  420 / 1000 | loss 2.7135\nstep  421 / 1000 | loss 2.5427\nstep  422 / 1000 | loss 2.4804\nstep  423 / 1000 | loss 1.9508\nstep  424 / 1000 | loss 2.5618\nstep  425 / 1000 | loss 2.6098\nstep  426 / 1000 | loss 2.8338\nstep  427 / 1000 | loss 2.4871\nstep  428 / 1000 | loss 2.3602\nstep  429 / 1000 | loss 2.0358\nstep  430 / 1000 | loss 2.3998\nstep  431 / 1000 | loss 2.1980\nstep  432 / 1000 | loss 2.0428\nstep  433 / 1000 | loss 2.3457\nstep  434 / 1000 | loss 2.3509\nstep  435 / 1000 | loss 2.4827\nstep  436 / 1000 | loss 3.3131\nstep  437 / 1000 | loss 2.7833\nstep  438 / 1000 | loss 1.8821\nstep  439 / 1000 | loss 1.9444\nstep  440 / 1000 | loss 2.1377\nstep  441 / 1000 | loss 2.6178\nstep  442 / 1000 | loss 3.1046\nstep  443 / 1000 | loss 3.2299\nstep  444 / 1000 | loss 2.3159\nstep  445 / 1000 | loss 2.3500\nstep  446 / 1000 | loss 2.0550\nstep  447 / 1000 | loss 2.0598\nstep  448 / 1000 | loss 3.0721\nstep  449 / 1000 | loss 2.1660\nstep  450 / 1000 | loss 3.0903\nstep  451 / 1000 | loss 2.8054\nstep  452 / 1000 | loss 2.8289\nstep  453 / 1000 | loss 2.5301\nstep  454 / 1000 | loss 1.9576\nstep  455 / 1000 | loss 2.2373\nstep  456 / 1000 | loss 2.7489\nstep  457 / 1000 | loss 2.5852\nstep  458 / 1000 | loss 2.7530\nstep  459 / 1000 | loss 1.7114\nstep  460 / 1000 | loss 2.3958\nstep  461 / 1000 | loss 1.8254\nstep  462 / 1000 | loss 2.7834\nstep  463 / 1000 | loss 2.0794\nstep  464 / 1000 | loss 2.2029\nstep  465 / 1000 | loss 2.7421\nstep  466 / 1000 | loss 2.2871\nstep  467 / 1000 | loss 2.2345\nstep  468 / 1000 | loss 2.2340\nstep  469 / 1000 | loss 2.3651\nstep  470 / 1000 | loss 3.8820\nstep  471 / 1000 | loss 2.5910\nstep  472 / 1000 | loss 2.7750\nstep  473 / 1000 | loss 2.6283\nstep  474 / 1000 | loss 2.3571\nstep  475 / 1000 | loss 2.4745\nstep  476 / 1000 | loss 2.1805\nstep  477 / 1000 | loss 2.9413\nstep  478 / 1000 | loss 2.2745\nstep  479 / 1000 | loss 2.0886\nstep  480 / 1000 | loss 2.0120\nstep  481 / 1000 | loss 2.9853\nstep  482 / 1000 | loss 2.7189\nstep  483 / 1000 | loss 2.3466\nstep  484 / 1000 | loss 2.8693\nstep  485 / 1000 | loss 2.4805\nstep  486 / 1000 | loss 2.1715\nstep  487 / 1000 | loss 2.7516\nstep  488 / 1000 | loss 2.6655\nstep  489 / 1000 | loss 2.3425\nstep  490 / 1000 | loss 2.2978\nstep  491 / 1000 | loss 2.2573\nstep  492 / 1000 | loss 2.3424\nstep  493 / 1000 | loss 2.4360\nstep  494 / 1000 | loss 2.1313\nstep  495 / 1000 | loss 2.4870\nstep  496 / 1000 | loss 2.5856\nstep  497 / 1000 | loss 2.9952\nstep  498 / 1000 | loss 2.4689\nstep  499 / 1000 | loss 2.2353\nstep  500 / 1000 | loss 2.0645\nstep  501 / 1000 | loss 2.4261\nstep  502 / 1000 | loss 2.1254\nstep  503 / 1000 | loss 2.7352\nstep  504 / 1000 | loss 2.0662\nstep  505 / 1000 | loss 2.3327\nstep  506 / 1000 | loss 2.4337\nstep  507 / 1000 | loss 2.4315\nstep  508 / 1000 | loss 2.6284\nstep  509 / 1000 | loss 2.8761\nstep  510 / 1000 | loss 2.7854\nstep  511 / 1000 | loss 2.2922\nstep  512 / 1000 | loss 2.2573\nstep  513 / 1000 | loss 2.4115\nstep  514 / 1000 | loss 2.8810\nstep  515 / 1000 | loss 2.6771\nstep  516 / 1000 | loss 3.0052\nstep  517 / 1000 | loss 2.1366\nstep  518 / 1000 | loss 2.2575\nstep  519 / 1000 | loss 2.0644\nstep  520 / 1000 | loss 2.7970\nstep  521 / 1000 | loss 1.6685\nstep  522 / 1000 | loss 1.8816\nstep  523 / 1000 | loss 2.1512\nstep  524 / 1000 | loss 2.4364\nstep  525 / 1000 | loss 2.3002\nstep  526 / 1000 | loss 2.6904\nstep  527 / 1000 | loss 1.7979\nstep  528 / 1000 | loss 2.5294\nstep  529 / 1000 | loss 2.3032\nstep  530 / 1000 | loss 1.6063\nstep  531 / 1000 | loss 2.5921\nstep  532 / 1000 | loss 2.3464\nstep  533 / 1000 | loss 3.5815\nstep  534 / 1000 | loss 2.2109\nstep  535 / 1000 | loss 3.1679\nstep  536 / 1000 | loss 1.8492\nstep  537 / 1000 | loss 1.5782\nstep  538 / 1000 | loss 2.4474\nstep  539 / 1000 | loss 1.8286\nstep  540 / 1000 | loss 2.7201\nstep  541 / 1000 | loss 2.7791\nstep  542 / 1000 | loss 1.9045\nstep  543 / 1000 | loss 3.2878\nstep  544 / 1000 | loss 2.3980\nstep  545 / 1000 | loss 2.8266\nstep  546 / 1000 | loss 2.4227\nstep  547 / 1000 | loss 2.1204\nstep  548 / 1000 | loss 2.8575\nstep  549 / 1000 | loss 2.0631\nstep  550 / 1000 | loss 1.9310\nstep  551 / 1000 | loss 2.6828\nstep  552 / 1000 | loss 2.4919\nstep  553 / 1000 | loss 2.5412\nstep  554 / 1000 | loss 2.7195\nstep  555 / 1000 | loss 2.9065\nstep  556 / 1000 | loss 2.3740\nstep  557 / 1000 | loss 2.5296\nstep  558 / 1000 | loss 1.9853\nstep  559 / 1000 | loss 2.5890\nstep  560 / 1000 | loss 3.1969\nstep  561 / 1000 | loss 1.8082\nstep  562 / 1000 | loss 2.9966\nstep  563 / 1000 | loss 2.3597\nstep  564 / 1000 | loss 2.0989\nstep  565 / 1000 | loss 3.0321\nstep  566 / 1000 | loss 1.7108\nstep  567 / 1000 | loss 2.5155\nstep  568 / 1000 | loss 2.7469\nstep  569 / 1000 | loss 2.5179\nstep  570 / 1000 | loss 2.8211\nstep  571 / 1000 | loss 1.9473\nstep  572 / 1000 | loss 3.1226\nstep  573 / 1000 | loss 3.0085\nstep  574 / 1000 | loss 2.4998\nstep  575 / 1000 | loss 2.2788\nstep  576 / 1000 | loss 2.0708\nstep  577 / 1000 | loss 1.9976\nstep  578 / 1000 | loss 2.6646\nstep  579 / 1000 | loss 2.0727\nstep  580 / 1000 | loss 2.4337\nstep  581 / 1000 | loss 2.3260\nstep  582 / 1000 | loss 2.4394\nstep  583 / 1000 | loss 2.7697\nstep  584 / 1000 | loss 2.9226\nstep  585 / 1000 | loss 2.2863\nstep  586 / 1000 | loss 2.4610\nstep  587 / 1000 | loss 2.1763\nstep  588 / 1000 | loss 2.0816\nstep  589 / 1000 | loss 1.8776\nstep  590 / 1000 | loss 2.4569\nstep  591 / 1000 | loss 2.4763\nstep  592 / 1000 | loss 2.1966\nstep  593 / 1000 | loss 2.3452\nstep  594 / 1000 | loss 2.8074\nstep  595 / 1000 | loss 2.4341\nstep  596 / 1000 | loss 2.1553\nstep  597 / 1000 | loss 2.6157\nstep  598 / 1000 | loss 2.1024\nstep  599 / 1000 | loss 2.3983\nstep  600 / 1000 | loss 2.4851\nstep  601 / 1000 | loss 2.1083\nstep  602 / 1000 | loss 2.4919\nstep  603 / 1000 | loss 2.7452\nstep  604 / 1000 | loss 2.0589\nstep  605 / 1000 | loss 2.7860\nstep  606 / 1000 | loss 1.7675\nstep  607 / 1000 | loss 2.7445\nstep  608 / 1000 | loss 2.2072\nstep  609 / 1000 | loss 2.3056\nstep  610 / 1000 | loss 2.4470\nstep  611 / 1000 | loss 2.6861\nstep  612 / 1000 | loss 2.5383\nstep  613 / 1000 | loss 1.9791\nstep  614 / 1000 | loss 2.1122\nstep  615 / 1000 | loss 2.4416\nstep  616 / 1000 | loss 2.9865\nstep  617 / 1000 | loss 2.7236\nstep  618 / 1000 | loss 2.3293\nstep  619 / 1000 | loss 2.4571\nstep  620 / 1000 | loss 2.6560\nstep  621 / 1000 | loss 1.8379\nstep  622 / 1000 | loss 2.2556\nstep  623 / 1000 | loss 2.0642\nstep  624 / 1000 | loss 2.4819\nstep  625 / 1000 | loss 1.7747\nstep  626 / 1000 | loss 2.5039\nstep  627 / 1000 | loss 2.0995\nstep  628 / 1000 | loss 2.2031\nstep  629 / 1000 | loss 2.6526\nstep  630 / 1000 | loss 2.6197\nstep  631 / 1000 | loss 3.0481\nstep  632 / 1000 | loss 1.7443\nstep  633 / 1000 | loss 2.6695\nstep  634 / 1000 | loss 2.5338\nstep  635 / 1000 | loss 3.2450\nstep  636 / 1000 | loss 2.8575\nstep  637 / 1000 | loss 2.5257\nstep  638 / 1000 | loss 2.2855\nstep  639 / 1000 | loss 2.6202\nstep  640 / 1000 | loss 1.9703\nstep  641 / 1000 | loss 2.2895\nstep  642 / 1000 | loss 1.9095\nstep  643 / 1000 | loss 2.5737\nstep  644 / 1000 | loss 2.2433\nstep  645 / 1000 | loss 2.3000\nstep  646 / 1000 | loss 2.0239\nstep  647 / 1000 | loss 2.3138\nstep  648 / 1000 | loss 3.1185\nstep  649 / 1000 | loss 2.1672\nstep  650 / 1000 | loss 2.6138\nstep  651 / 1000 | loss 2.4730\nstep  652 / 1000 | loss 2.4868\nstep  653 / 1000 | loss 2.3750\nstep  654 / 1000 | loss 2.1639\nstep  655 / 1000 | loss 3.0494\nstep  656 / 1000 | loss 2.4772\nstep  657 / 1000 | loss 2.1428\nstep  658 / 1000 | loss 2.9535\nstep  659 / 1000 | loss 2.5928\nstep  660 / 1000 | loss 2.4115\nstep  661 / 1000 | loss 2.1242\nstep  662 / 1000 | loss 2.9471\nstep  663 / 1000 | loss 2.6772\nstep  664 / 1000 | loss 2.6958\nstep  665 / 1000 | loss 2.4493\nstep  666 / 1000 | loss 2.0646\nstep  667 / 1000 | loss 2.9612\nstep  668 / 1000 | loss 2.8441\nstep  669 / 1000 | loss 2.1719\nstep  670 / 1000 | loss 2.1952\nstep  671 / 1000 | loss 2.1350\nstep  672 / 1000 | loss 1.8856\nstep  673 / 1000 | loss 2.5404\nstep  674 / 1000 | loss 2.4887\nstep  675 / 1000 | loss 2.7627\nstep  676 / 1000 | loss 2.1296\nstep  677 / 1000 | loss 2.0944\nstep  678 / 1000 | loss 2.2733\nstep  679 / 1000 | loss 2.3283\nstep  680 / 1000 | loss 2.2191\nstep  681 / 1000 | loss 2.9738\nstep  682 / 1000 | loss 2.0353\nstep  683 / 1000 | loss 1.5894\nstep  684 / 1000 | loss 2.3880\nstep  685 / 1000 | loss 1.8963\nstep  686 / 1000 | loss 2.4264\nstep  687 / 1000 | loss 1.8933\nstep  688 / 1000 | loss 2.3557\nstep  689 / 1000 | loss 2.3917\nstep  690 / 1000 | loss 2.3202\nstep  691 / 1000 | loss 2.0521\nstep  692 / 1000 | loss 1.8742\nstep  693 / 1000 | loss 2.1245\nstep  694 / 1000 | loss 3.7008\nstep  695 / 1000 | loss 2.7782\nstep  696 / 1000 | loss 2.4651\nstep  697 / 1000 | loss 3.2385\nstep  698 / 1000 | loss 2.6590\nstep  699 / 1000 | loss 2.6012\nstep  700 / 1000 | loss 2.3357\nstep  701 / 1000 | loss 2.1908\nstep  702 / 1000 | loss 3.2303\nstep  703 / 1000 | loss 2.5401\nstep  704 / 1000 | loss 2.0141\nstep  705 / 1000 | loss 2.2466\nstep  706 / 1000 | loss 2.2559\nstep  707 / 1000 | loss 2.6487\nstep  708 / 1000 | loss 2.7316\nstep  709 / 1000 | loss 2.0201\nstep  710 / 1000 | loss 2.2398\nstep  711 / 1000 | loss 2.8304\nstep  712 / 1000 | loss 2.4438\nstep  713 / 1000 | loss 2.4199\nstep  714 / 1000 | loss 2.5542\nstep  715 / 1000 | loss 1.9634\nstep  716 / 1000 | loss 1.8876\nstep  717 / 1000 | loss 2.1661\nstep  718 / 1000 | loss 2.0400\nstep  719 / 1000 | loss 2.6692\nstep  720 / 1000 | loss 2.1266\nstep  721 / 1000 | loss 2.1274\nstep  722 / 1000 | loss 2.6668\nstep  723 / 1000 | loss 2.1620\nstep  724 / 1000 | loss 2.7405\nstep  725 / 1000 | loss 2.8878\nstep  726 / 1000 | loss 2.6247\nstep  727 / 1000 | loss 1.7349\nstep  728 / 1000 | loss 2.1850\nstep  729 / 1000 | loss 2.2787\nstep  730 / 1000 | loss 2.2568\nstep  731 / 1000 | loss 2.5408\nstep  732 / 1000 | loss 2.5605\nstep  733 / 1000 | loss 2.5687\nstep  734 / 1000 | loss 2.9981\nstep  735 / 1000 | loss 3.1957\nstep  736 / 1000 | loss 2.4961\nstep  737 / 1000 | loss 3.1245\nstep  738 / 1000 | loss 1.8570\nstep  739 / 1000 | loss 2.1931\nstep  740 / 1000 | loss 3.2648\nstep  741 / 1000 | loss 2.7264\nstep  742 / 1000 | loss 2.7551\nstep  743 / 1000 | loss 2.4624\nstep  744 / 1000 | loss 2.4762\nstep  745 / 1000 | loss 2.1545\nstep  746 / 1000 | loss 2.8443\nstep  747 / 1000 | loss 2.7363\nstep  748 / 1000 | loss 2.8508\nstep  749 / 1000 | loss 2.4379\nstep  750 / 1000 | loss 2.0780\nstep  751 / 1000 | loss 2.3346\nstep  752 / 1000 | loss 1.8021\nstep  753 / 1000 | loss 3.0455\nstep  754 / 1000 | loss 2.4193\nstep  755 / 1000 | loss 2.6941\nstep  756 / 1000 | loss 2.6088\nstep  757 / 1000 | loss 2.4175\nstep  758 / 1000 | loss 2.3642\nstep  759 / 1000 | loss 2.2976\nstep  760 / 1000 | loss 2.6314\nstep  761 / 1000 | loss 2.4459\nstep  762 / 1000 | loss 2.1060\nstep  763 / 1000 | loss 2.1103\nstep  764 / 1000 | loss 2.0754\nstep  765 / 1000 | loss 2.0023\nstep  766 / 1000 | loss 2.4650\nstep  767 / 1000 | loss 2.7972\nstep  768 / 1000 | loss 2.5172\nstep  769 / 1000 | loss 1.9687\nstep  770 / 1000 | loss 2.2378\nstep  771 / 1000 | loss 2.4757\nstep  772 / 1000 | loss 2.0182\nstep  773 / 1000 | loss 1.7604\nstep  774 / 1000 | loss 2.6085\nstep  775 / 1000 | loss 2.8517\nstep  776 / 1000 | loss 1.8031\nstep  777 / 1000 | loss 2.8917\nstep  778 / 1000 | loss 2.0491\nstep  779 / 1000 | loss 2.7976\nstep  780 / 1000 | loss 2.0455\nstep  781 / 1000 | loss 1.8593\nstep  782 / 1000 | loss 2.8120\nstep  783 / 1000 | loss 1.7626\nstep  784 / 1000 | loss 2.4415\nstep  785 / 1000 | loss 2.2726\nstep  786 / 1000 | loss 2.1308\nstep  787 / 1000 | loss 2.6911\nstep  788 / 1000 | loss 2.7761\nstep  789 / 1000 | loss 1.9803\nstep  790 / 1000 | loss 2.2568\nstep  791 / 1000 | loss 2.1672\nstep  792 / 1000 | loss 2.1728\nstep  793 / 1000 | loss 2.1669\nstep  794 / 1000 | loss 2.2664\nstep  795 / 1000 | loss 1.9224\nstep  796 / 1000 | loss 2.4312\nstep  797 / 1000 | loss 2.1678\nstep  798 / 1000 | loss 2.7734\nstep  799 / 1000 | loss 1.8007\nstep  800 / 1000 | loss 2.2632\nstep  801 / 1000 | loss 2.1064\nstep  802 / 1000 | loss 2.1354\nstep  803 / 1000 | loss 1.8741\nstep  804 / 1000 | loss 1.9609\nstep  805 / 1000 | loss 2.6330\nstep  806 / 1000 | loss 2.1938\nstep  807 / 1000 | loss 2.1032\nstep  808 / 1000 | loss 1.9303\nstep  809 / 1000 | loss 2.2945\nstep  810 / 1000 | loss 2.0318\nstep  811 / 1000 | loss 2.0004\nstep  812 / 1000 | loss 2.3280\nstep  813 / 1000 | loss 2.4433\nstep  814 / 1000 | loss 2.2201\nstep  815 / 1000 | loss 2.2991\nstep  816 / 1000 | loss 2.7418\nstep  817 / 1000 | loss 1.7048\nstep  818 / 1000 | loss 2.5284\nstep  819 / 1000 | loss 1.9636\nstep  820 / 1000 | loss 2.4420\nstep  821 / 1000 | loss 2.2038\nstep  822 / 1000 | loss 2.5172\nstep  823 / 1000 | loss 1.7077\nstep  824 / 1000 | loss 2.2398\nstep  825 / 1000 | loss 2.8151\nstep  826 / 1000 | loss 2.4977\nstep  827 / 1000 | loss 2.6141\nstep  828 / 1000 | loss 2.7821\nstep  829 / 1000 | loss 1.8019\nstep  830 / 1000 | loss 2.4835\nstep  831 / 1000 | loss 2.0712\nstep  832 / 1000 | loss 2.0766\nstep  833 / 1000 | loss 1.8469\nstep  834 / 1000 | loss 2.2951\nstep  835 / 1000 | loss 2.4414\nstep  836 / 1000 | loss 2.5103\nstep  837 / 1000 | loss 3.3694\nstep  838 / 1000 | loss 2.3500\nstep  839 / 1000 | loss 2.3950\nstep  840 / 1000 | loss 2.5399\nstep  841 / 1000 | loss 2.9150\nstep  842 / 1000 | loss 2.4967\nstep  843 / 1000 | loss 1.9816\nstep  844 / 1000 | loss 2.6846\nstep  845 / 1000 | loss 2.5020\nstep  846 / 1000 | loss 1.8127\nstep  847 / 1000 | loss 2.8528\nstep  848 / 1000 | loss 2.0746\nstep  849 / 1000 | loss 1.5794\nstep  850 / 1000 | loss 2.4860\nstep  851 / 1000 | loss 2.7039\nstep  852 / 1000 | loss 2.1478\nstep  853 / 1000 | loss 2.3845\nstep  854 / 1000 | loss 2.3782\nstep  855 / 1000 | loss 2.3659\nstep  856 / 1000 | loss 2.1089\nstep  857 / 1000 | loss 2.8112\nstep  858 / 1000 | loss 2.7589\nstep  859 / 1000 | loss 2.1425\nstep  860 / 1000 | loss 2.4466\nstep  861 / 1000 | loss 2.6435\nstep  862 / 1000 | loss 2.6565\nstep  863 / 1000 | loss 2.5271\nstep  864 / 1000 | loss 3.1404\nstep  865 / 1000 | loss 2.0112\nstep  866 / 1000 | loss 2.0564\nstep  867 / 1000 | loss 2.1266\nstep  868 / 1000 | loss 1.8993\nstep  869 / 1000 | loss 2.4955\nstep  870 / 1000 | loss 2.7364\nstep  871 / 1000 | loss 2.2273\nstep  872 / 1000 | loss 2.3312\nstep  873 / 1000 | loss 2.7687\nstep  874 / 1000 | loss 2.2820\nstep  875 / 1000 | loss 2.2595\nstep  876 / 1000 | loss 2.3459\nstep  877 / 1000 | loss 2.0663\nstep  878 / 1000 | loss 2.7865\nstep  879 / 1000 | loss 2.1826\nstep  880 / 1000 | loss 2.6298\nstep  881 / 1000 | loss 2.3814\nstep  882 / 1000 | loss 1.8578\nstep  883 / 1000 | loss 2.1931\nstep  884 / 1000 | loss 2.1980\nstep  885 / 1000 | loss 2.2070\nstep  886 / 1000 | loss 2.1261\nstep  887 / 1000 | loss 3.0004\nstep  888 / 1000 | loss 2.2790\nstep  889 / 1000 | loss 2.6385\nstep  890 / 1000 | loss 2.0798\nstep  891 / 1000 | loss 2.1188\nstep  892 / 1000 | loss 3.4579\nstep  893 / 1000 | loss 2.0826\nstep  894 / 1000 | loss 1.7378\nstep  895 / 1000 | loss 2.0197\nstep  896 / 1000 | loss 2.4508\nstep  897 / 1000 | loss 2.2737\nstep  898 / 1000 | loss 1.9217\nstep  899 / 1000 | loss 2.2933\nstep  900 / 1000 | loss 2.7785\nstep  901 / 1000 | loss 2.0881\nstep  902 / 1000 | loss 2.3490\nstep  903 / 1000 | loss 1.7459\nstep  904 / 1000 | loss 2.0612\nstep  905 / 1000 | loss 2.1511\nstep  906 / 1000 | loss 1.9278\nstep  907 / 1000 | loss 2.6180\nstep  908 / 1000 | loss 2.3714\nstep  909 / 1000 | loss 2.2607\nstep  910 / 1000 | loss 2.7556\nstep  911 / 1000 | loss 2.2940\nstep  912 / 1000 | loss 2.6726\nstep  913 / 1000 | loss 2.4291\nstep  914 / 1000 | loss 2.8404\nstep  915 / 1000 | loss 2.2663\nstep  916 / 1000 | loss 2.3037\nstep  917 / 1000 | loss 2.2782\nstep  918 / 1000 | loss 2.4194\nstep  919 / 1000 | loss 2.4164\nstep  920 / 1000 | loss 2.6305\nstep  921 / 1000 | loss 1.9157\nstep  922 / 1000 | loss 1.8924\nstep  923 / 1000 | loss 2.0604\nstep  924 / 1000 | loss 2.5970\nstep  925 / 1000 | loss 2.1268\nstep  926 / 1000 | loss 2.0386\nstep  927 / 1000 | loss 2.5987\nstep  928 / 1000 | loss 2.3180\nstep  929 / 1000 | loss 1.8104\nstep  930 / 1000 | loss 2.4971\nstep  931 / 1000 | loss 3.1351\nstep  932 / 1000 | loss 2.3636\nstep  933 / 1000 | loss 2.4958\nstep  934 / 1000 | loss 2.1538\nstep  935 / 1000 | loss 2.0586\nstep  936 / 1000 | loss 1.8687\nstep  937 / 1000 | loss 1.8116\nstep  938 / 1000 | loss 1.6251\nstep  939 / 1000 | loss 1.9955\nstep  940 / 1000 | loss 1.7995\nstep  941 / 1000 | loss 1.9697\nstep  942 / 1000 | loss 2.1796\nstep  943 / 1000 | loss 1.9453\nstep  944 / 1000 | loss 2.6730\nstep  945 / 1000 | loss 2.1508\nstep  946 / 1000 | loss 2.3271\nstep  947 / 1000 | loss 2.0929\nstep  948 / 1000 | loss 1.7849\nstep  949 / 1000 | loss 1.9801\nstep  950 / 1000 | loss 2.3016\nstep  951 / 1000 | loss 2.7790\nstep  952 / 1000 | loss 2.0783\nstep  953 / 1000 | loss 2.2319\nstep  954 / 1000 | loss 2.1295\nstep  955 / 1000 | loss 2.5928\nstep  956 / 1000 | loss 3.0061\nstep  957 / 1000 | loss 2.1160\nstep  958 / 1000 | loss 2.2593\nstep  959 / 1000 | loss 2.0209\nstep  960 / 1000 | loss 2.1214\nstep  961 / 1000 | loss 2.2633\nstep  962 / 1000 | loss 2.3385\nstep  963 / 1000 | loss 2.5537\nstep  964 / 1000 | loss 2.7235\nstep  965 / 1000 | loss 3.3042\nstep  966 / 1000 | loss 2.1621\nstep  967 / 1000 | loss 2.9326\nstep  968 / 1000 | loss 1.8063\nstep  969 / 1000 | loss 2.2380\nstep  970 / 1000 | loss 1.9579\nstep  971 / 1000 | loss 2.3572\nstep  972 / 1000 | loss 2.1710\nstep  973 / 1000 | loss 2.5142\nstep  974 / 1000 | loss 2.0779\nstep  975 / 1000 | loss 1.9271\nstep  976 / 1000 | loss 2.0277\nstep  977 / 1000 | loss 2.5328\nstep  978 / 1000 | loss 1.8817\nstep  979 / 1000 | loss 1.9636\nstep  980 / 1000 | loss 1.9525\nstep  981 / 1000 | loss 2.4269\nstep  982 / 1000 | loss 2.8226\nstep  983 / 1000 | loss 2.4713\nstep  984 / 1000 | loss 2.0303\nstep  985 / 1000 | loss 2.7422\nstep  986 / 1000 | loss 2.6811\nstep  987 / 1000 | loss 1.9173\nstep  988 / 1000 | loss 2.4303\nstep  989 / 1000 | loss 2.4466\nstep  990 / 1000 | loss 2.6354\nstep  991 / 1000 | loss 2.1729\nstep  992 / 1000 | loss 1.9659\nstep  993 / 1000 | loss 2.4409\nstep  994 / 1000 | loss 1.9618\nstep  995 / 1000 | loss 2.5188\nstep  996 / 1000 | loss 2.1018\nstep  997 / 1000 | loss 1.7791\nstep  998 / 1000 | loss 2.4764\nstep  999 / 1000 | loss 2.4730\nstep 1000 / 1000 | loss 2.6497\n\n--- inference (new, hallucinated names) ---\nsample  1: kamon\nsample  2: ann\nsample  3: karai\nsample  4: jaire\nsample  5: vialan\nsample  6: karia\nsample  7: yeran\nsample  8: anna\nsample  9: areli\nsample 10: kaina\nsample 11: konna\nsample 12: keylen\nsample 13: liole\nsample 14: alerin\nsample 15: earan\nsample 16: lenne\nsample 17: kana\nsample 18: lara\nsample 19: alela\nsample 20: anton\ntrain start:  01:53:03\ntrain end:    01:56:23\nfinal end:    01:56:25\ntrain time:   199.7s  (3.3 min)\ntotal time:   201.5s  (3.4 min)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## **total time: approx. 3.4 mins**","metadata":{}},{"cell_type":"markdown","source":"# 2. My microGPT version\n---\n\nProject can be found on [GitHub](https://github.com/chizkidd/microGPT/blob/main/run_microgpt.py). I made improvements to the [Karpathy version](https://x.com/karpathy/status/2021694437152157847):\n\n1. PyTorch/GPU port with multi-dataset runner\n2. Gradient clipping\n3. EMA loss smoothing\n4. Train/val split (90/10)\n5. Top-k sampling\n6. Seeded `generate(prompt)`\n7. Checkpointing (save/resume)\n8. Live loss plots (train raw, EMA, val)\n9. 10-line progress logging per dataset\n10. Stream vs discrete inference mode\n11. Full CLI â€” all hyperparameters overridable from command line (`--n-embd`, `--lr`, `--beta1`, `--beta2`, `--steps` and more)\n\n>Datasets: names, PokÃ©mon, cities, English words, Paul Graham essays, Shakespeare.\n\n","metadata":{}},{"cell_type":"code","source":"!find /kaggle/input/models/chizkidd -type f","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T02:36:58.928697Z","iopub.execute_input":"2026-02-15T02:36:58.929286Z","iopub.status.idle":"2026-02-15T02:36:59.056667Z","shell.execute_reply.started":"2026-02-15T02:36:58.92925Z","shell.execute_reply":"2026-02-15T02:36:59.055938Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/models/chizkidd/run-microgpt-v2-py/pytorch/default/1/run_microgpt3.py\n/kaggle/input/models/chizkidd/run-microgpt-py/pytorch/default/1/run_microgpt2.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_start = time.time()\n\n!cp /kaggle/input/models/chizkidd/run-microgpt-v2-py/pytorch/default/1/run_microgpt3.py /kaggle/working/\n!python /kaggle/working/run_microgpt3.py \\\n    --only names \\\n    --n-embd 16 \\\n    --n-head 4 \\\n    --n-layer 1 \\\n    --block-size 16 \\\n    --lr 0.01 \\\n    --beta1 0.85 \\\n    --beta2 0.99 \\\n    --eps 1e-8 \\\n    --steps 1000\n\n\nfinal_end = time.time()\n\nprint(f\"train start:  {time.strftime('%H:%M:%S', time.localtime(train_start))}\")\n# print(f\"train end:    {time.strftime('%H:%M:%S', time.localtime(train_end))}\")\nprint(f\"final end:    {time.strftime('%H:%M:%S', time.localtime(final_end))}\")\n# print(f\"train time:   {train_end - train_start:.1f}s  ({(train_end - train_start)/60:.1f} min)\")\nprint(f\"total time:   {final_end - train_start:.1f}s  ({(final_end - train_start)/60:.1f} min)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T02:50:07.690939Z","iopub.execute_input":"2026-02-15T02:50:07.691762Z","iopub.status.idle":"2026-02-15T02:50:31.448955Z","shell.execute_reply.started":"2026-02-15T02:50:07.69173Z","shell.execute_reply":"2026-02-15T02:50:31.448242Z"}},"outputs":[{"name":"stdout","text":"running 1 dataset(s): ['names']\ncli overrides: {'steps': 1000, 'n_embd': 16, 'n_layer': 1, 'n_head': 4, 'block_size': 16, 'lr': 0.01, 'beta1': 0.85, 'beta2': 0.99, 'eps': 1e-08}\n\n============================================================\n  Dataset : names\n  Note    : Baby names -- the original benchmark\n  Steps   : 1000  |  n_embd=16  n_layer=1  n_head=4\n============================================================\ndevice: cuda\n  [cache] datasets/names.txt\nnum docs: 32033\ntrain: 28829  |  val: 3204\nvocab size: 27\nnum params: 4,224\n  [names] step  100/1000 ( 10%)  loss 3.2984  ema 2.7300  val 2.7650\n  [names] step  200/1000 ( 20%)  loss 2.3922  ema 2.5703  val 2.4694\n  [names] step  300/1000 ( 30%)  loss 2.4642  ema 2.4706  val 2.6220\n  [names] step  400/1000 ( 40%)  loss 2.4347  ema 2.5199  val 2.5021\n  [names] step  500/1000 ( 50%)  loss 1.9637  ema 2.4574  val 2.5669\n  [names] step  600/1000 ( 60%)  loss 2.4858  ema 2.4661  val 2.4196\n  [names] step  700/1000 ( 70%)  loss 2.2220  ema 2.4157  val 2.5920\n  [names] step  800/1000 ( 80%)  loss 2.2434  ema 2.3328  val 2.2287\n  [names] step  900/1000 ( 90%)  loss 2.9282  ema 2.3648  val 2.4373\n  [names] step 1000/1000 (100%)  loss 2.5973  ema 2.3426  val 2.3766\n  loss plot  -> outputs/names/loss.png\n  sample  1: kaita\n  sample  2: elyvi\n  sample  3: aneen\n  sample  4: adaran\n  sample  5: kakan\n  sample  6: risha\n  sample  7: onaion\n  sample  8: livi\n  sample  9: stili\n  sample 10: amara\n  sample 11: mier\n  sample 12: diylen\n  sample 13: onyan\n  sample 14: airiah\n  sample 15: sainen\n  sample 16: alylen\n  sample 17: darai\n  sample 18: malan\n  sample 19: kahay\n  sample 20: datir\n  seed 'a' -> anaria\n  seed 'b' -> belir\n  seed 'c' -> cedi\n  samples    -> outputs/names/samples.txt\n  checkpoint -> outputs/names/ckpt.pt\n\n==================================================\n  SUMMARY\n==================================================\n  dataset              final ema loss final val loss\n  ------------------------------------------------\n  names                        2.3426         2.3766\n==================================================\n\nAll outputs saved under: /kaggle/working/outputs/\n  combined plot -> outputs/all_losses.png\ntrain start:  02:50:07\nfinal end:    02:50:31\ntotal time:   23.8s  (0.4 min)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## **total time: approx. 0.4 mins**","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Dataset: Paul Graham essays","metadata":{}},{"cell_type":"code","source":"import time\n\nstart = time.time()\n\n!cp /kaggle/input/models/chizkidd/run-microgpt-v2-py/pytorch/default/1/run_microgpt3.py /kaggle/working/run_microgpt.py\n!python /kaggle/working/run_microgpt.py --only paul_graham --steps 100000\n\nend = time.time()\n\nprint(f\"start:   {time.strftime('%H:%M:%S', time.localtime(start))}\")\nprint(f\"end:     {time.strftime('%H:%M:%S', time.localtime(end))}\")\nprint(f\"elapsed: {end - start:.1f}s  ({(end - start)/60:.1f} min)\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pILw4DDOWwE","outputId":"03e9054b-3db4-46fb-bfe5-dc17a1cc3cd9","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T02:55:09.607705Z","iopub.execute_input":"2026-02-15T02:55:09.608532Z","iopub.status.idle":"2026-02-15T04:02:40.399888Z","shell.execute_reply.started":"2026-02-15T02:55:09.608499Z","shell.execute_reply":"2026-02-15T04:02:40.398971Z"}},"outputs":[{"name":"stdout","text":"running 1 dataset(s): ['paul_graham']\ncli overrides: {'steps': 100000}\n\n============================================================\n  Dataset : paul_graham\n  Note    : Paul Graham essays (~200 txt files) -- distinctive prose style\n  Steps   : 100000  |  n_embd=64  n_layer=4  n_head=4\n============================================================\ndevice: cuda\n  [download] GitHub file listing: https://api.github.com/repos/sgoel97/essay-datasets/contents/paul_graham_essays/text_data\n  [saved]    datasets/paul_graham_essays/_pg_index.json  (218 files)\n  loaded 9086 paragraphs from 218 PG essays\nnum docs: 9086\ntrain: 8177  |  val: 909\nvocab size: 103\nnum params: 218,496\n  [paul_graham] step 10000/100000 ( 10%)  loss 1.6247  ema 1.6569  val 1.6670\n  [paul_graham] step 20000/100000 ( 20%)  loss 1.3581  ema 1.4516  val 1.6669\n  [paul_graham] step 30000/100000 ( 30%)  loss 1.5422  ema 1.4516  val 1.9637\n  [paul_graham] step 40000/100000 ( 40%)  loss 1.3268  ema 1.4156  val 1.4407\n  [paul_graham] step 50000/100000 ( 50%)  loss 1.3737  ema 1.3804  val 1.4837\n  [paul_graham] step 60000/100000 ( 60%)  loss 1.2134  ema 1.3491  val 1.4855\n  [paul_graham] step 70000/100000 ( 70%)  loss 1.4297  ema 1.3315  val 1.4413\n  [paul_graham] step 80000/100000 ( 80%)  loss 1.1024  ema 1.2517  val 1.6893\n  [paul_graham] step 90000/100000 ( 90%)  loss 1.3578  ema 1.2468  val 1.3891\n  [paul_graham] step 100000/100000 (100%)  loss 1.0135  ema 1.2109  val 1.3040\n  loss plot  -> outputs/paul_graham/loss.png\n  seed '':\nIt's hardered to advantage of the companies software. I could had a particularly of startups will in a language to work them was people is another in a few a tried to be and in as people such it was the story one to the consist of clear of the country our and web-sackating that were the first ideas to angel why conceral the of say success,thisy are about it has and so one startup is that there's a startup the time the same always they were all the people who wanted for the startup that they were some ads, t\n\n  seed 'The ':\nThe money including is through in the reason at the really the short is one so probably is that they can't have making art. There's always in the people whose will in in effect them, and business happened it to that it's hard to get our startups to make out in a recessions. There is a millions and and something to take is an startup, founders of part that it important, they enlike to selling bad it is also at the beauted to growth of exped to raise and be on the way the schoolate of composed between every indiv\n\n  seed 'I ':\nI wish wish that you don't have to be at the term help we trev. When I think they have to be path control the best deal to say their way to have to do to for the things and the time, the problem of sities intervig the our cases with but town in mostly, though is there is a replace to angel of contination of the and that's angels. It'strinute to be situation a lot of suggests false the the core will was not secidate with interesting to world. In the country as the acquisities and and money startup century have\n\n  samples    -> outputs/paul_graham/samples.txt\n  checkpoint -> outputs/paul_graham/ckpt.pt\n\n==================================================\n  SUMMARY\n==================================================\n  dataset              final ema loss final val loss\n  ------------------------------------------------\n  paul_graham                  1.2109         1.3040\n==================================================\n\nAll outputs saved under: /kaggle/working/outputs/\n  combined plot -> outputs/all_losses.png\nstart:   02:55:09\nend:     04:02:40\nelapsed: 4050.8s  (67.5 min)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"id":"v0AmF-1KTbSZ","trusted":true},"outputs":[],"execution_count":null}]}